[
  {
    "id": "project-1",
    "title": "Wasserstein-GAN for Optimizing Drug Candidates",
    "image": "/img/project1.jpg",
    "description": "Developed a W-GAN model to optimize drug candidates.",
    "tags": [
      "Python",
      "PyTorch",
      "Drug Discovery"
    ],
    "github": "https://github.com/Kiarashmo/WGAN-for-Drug-Design",
    "details": "\n# Designing Drug Candidates with Generative Adversarial Networks\n\nThis project tackles the challenges of traditional drug discovery, which is often a slow, expensive, and inefficient process. Faced with a vast chemical space of up to \\(10^{60}\\) potential molecules, this research proposes a computational approach using deep learning to accelerate the design of new drug candidates. The core of this project is a generative model designed to produce novel, valid, and optimized molecules represented in SMILES (Simplified Molecular Input Line Entry Strings) format.\n\n## Model Architecture\n\n### Model Diagram:\n![Model Diagram](/img/diagram1.png)\n\n### VAE-WGAN-GP\nThe model combines a Variational Autoencoder (VAE) with a Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP). This hybrid architecture leverages the VAE's ability to represent discrete molecular structures in a continuous space, which is ideal for training the GAN.\n\n1. **Input**:\n   - The model is trained on a dataset of 100,000 SMILES strings extracted from the ChEMBL and Zinc Biogenic public databases.\n   - The generator component takes a random noise vector of size 256 as its initial input.\n\n2. **Preprocessing and VAE Encoder**:\n   - SMILES strings are first preprocessed through standardization, tokenization, and padding.\n   - The VAE's Encoder then transforms these strings into 256-dimensional continuous latent vectors, which serve as the \"real\" data for the GAN training.\n\n3. **WGAN-GP Core**:\n   - The Generator network attempts to produce realistic 256-dimensional latent vectors from the initial random noise.\n   - The Critic (Discriminator) is trained to distinguish between the real latent vectors (from the Encoder) and the fake ones (from the Generator). It uses a Wasserstein loss function with a gradient penalty to ensure training stability and avoid issues like vanishing gradients.\n\n4. **Decoder and Output**:\n   - Once the GAN is trained, the Generator can produce new, optimized latent vectors.\n   - These generated vectors are passed to the VAE's Decoder, which converts them back into novel SMILES strings, representing new potential drug candidates.\n\n## Results\nThe model was evaluated by generating 100 new molecules and assessing them on several key criteria. The primary evaluation metrics were **Validity**, **logP** (partition coefficient), and **Molecular Weight**. Quantitative analysis showed that the model's ability to generate valid molecules progressively increased with more training epochs. The generated molecules were further analyzed for their drug-like properties (logP and molecular weight) to determine their potential as effective drug candidates.\n\n### Explanation of Molecular Validity:\nA **Valid Molecule** refers to a generated SMILES string that represents a chemically correct and synthesizable structure. In this project, validity was confirmed using the RDKit software library. This is the most critical initial metric because it ensures that the model is not producing nonsensical or physically impossible chemical structures, making the generated candidates viable for further testing and consideration."
  },
  {
    "id": "project-2",
    "title": "Apnea Detection from Tracheal Sounds using ResNet",
    "image": "/img/project2.jpg",
    "description": "Developed a custom ResNet model to detect sleep apnea from tracheal sound recordings.",
    "tags": [
      "Python",
      "PyTorch",
      "Audio Processing"
    ],
    "github": "https://github.com/Kiarashmo/Apnea-Detection-with-AnisotropicResNet",
    "details": "\n# Apnea Detection with AnisotropicResNet\n\nThis project focuses on detecting sleep apnea episodes from tracheal sound recordings using deep learning. The AnisotropicResNet model is designed to process audio features such as Mel Spectrogram, MFCC, and Chroma Features extracted from tracheal sound data. The objective is to accurately predict periods of apnea by leveraging advanced feature extraction and a customized convolutional neural network architecture.\n\n## Model Architecture\n\n### Model Diagram:\n\n![Model Diagram](/img/diagram2.jpg)\n\n### AnisotropicResNet\nThe AnisotropicResNet model is a modified version of the ResNet-18 architecture, specifically tailored for the task of apnea detection. The model combines multiple audio features to form a 3-channel input, enabling the network to simultaneously process temporal and frequency information from the audio signals.\n\n1. **Input**:\n   - The model takes three key audio features as input:\n     - **Mel Spectrogram**: A visual representation of the power spectrum of audio signals on the mel scale.\n     - **MFCC (Mel Frequency Cepstral Coefficients)**: Captures short-term power spectrum of sound, frequently used in speech recognition.\n     - **Chroma Features**: Represents the energy distribution across 12 pitch classes (chromas), useful for capturing harmonic content.\n\n2. **Feature Reshaping**:\n   - To unify the dimensions of the input features, two fully connected layers are applied to reshape the MFCC and Chroma Features to match the size of the Mel Spectrogram. These reshaped features are then stacked to form a 3-channel input image.\n\n3. **Convolutional Layers**:\n   - The model applies two initial convolutional layers:\n     - **Temporal Convolution (1x7 kernel)**: Focuses on capturing patterns along the time axis.\n     - **Frequency Convolution (7x1 kernel)**: Captures frequency patterns from the audio data.\n\n4. **ResNet Backbone**:\n   - The modified ResNet-18 architecture processes the stacked 3-channel input, learning deep features from the combination of Mel Spectrogram, MFCC, and Chroma.\n\n5. **Output**:\n   - The final output layer generates predictions for apnea episodes based on the given time windows, leveraging the deep features learned by the network.\n\n\n## Results\nThe model was trained and evaluated on real tracheal sound data with various patients. The primary evaluation metrics include **Precision**, **Recall**, and **F1 Score**, which measure the accuracy of predicting apnea episodes. The model performance was analyzed both quantitatively and qualitatively, showing that it is capable of identifying apnea regions with a high degree of accuracy.\n\n### Explanation of True Positive (TP):\nA **True Positive (TP)** refers to a situation where the predicted apnea region has an Intersection over Union (IOU) greater than 70% with the actual ground truth apnea region. This threshold ensures that the predicted region significantly overlaps with the true apnea region, making it a valid and accurate prediction.\n\n| Model | Precision | Recall | F1 Score | Error |\n|:---|:---|:---|:---|:---|\n| Model 10 | 1.0 | 0.8 | 0.89 | 6.5 |\n| Model 2 | 0.8 | 0.67 | 0.73 | 4.25 |\n| Model 7 | 0.67 | 0.67 | 0.67 | 4.0 |\n| Model 5 | 0.67 | 0.6 | 0.63 | 3.17 |\n| Model 4 | 0.7 | 0.5 | 0.58 | 6.14 |"
  },
  {
    "id": "project-3",
    "title": "Stroke Motor-Imagery Classification via Transfer Learning",
    "image": "/img/project3.jpg",
    "description": "Developed the EEGNet model to classify motor imagery (MI) in stroke patients by applying transfer learning.",
    "tags": [
      "Python",
      "EEG",
      "Transfer Learning"
    ],
    "github": "https://github.com/Kiarashmo/Stroke-Patient-Motor-Imagery-Classification-System-by-Utilizing-Transfer-Learning",
    "details": "\r\n# Stroke Patient Motor-Imagery Classification using Transfer Learning\r\n\r\n## Overview\r\nBrain-computer interfaces (BCI), powered by the classification of brain signals such as electroencephalography (EEG), can potentially revolutionize how we interact with computers and the world around us. One of the main applications of these systems is to be used with Motor Imagery (MI) data, in order to detect the movement imagination of a subject without any actual movement, which can then be employed to recover the motor ability of stroke patients, by enabling control of assistive devices like robotic arms. However, a significant challenge in studying stroke patients lies in the scarcity of available MI data.\r\n\r\nGiven the abundance of large-scale and accessible datasets from healthy subjects, we aimed to investigate whether a model trained on healthy individuals' brain data could help overcome the shortage of stroke patients' data and improve the classification of their imagery movements. To address this, we formulated the problem as a binary hand movement imagery classification task, focusing on distinguishing between imagining the opening and closing of the left or right fist. In the beginning, we utilized EEGNet as the base model of our pipeline and trained it initially on healthy subjects' motor imagery dataset. Following this initial training, we employed transfer learning techniques and fine-tuned the model for our stroke patients dataset.\r\n\r\n![Model overview](/img/diagram3.png)\r\n\r\n## Our Task\r\nClassify whether the brain signal corresponds to the left or right hand\r\n\r\n## Results\r\nOur initial results suggest that our method achieved an accuracy of 90% on the healthy dataset. However, the accuracy observed on the stroke patient dataset was average. We anticipate seeing enhanced results after doing some improvements in preprocessing and hyperparameter tuning. In addition, we would like to explore the effect of the EEG channel selection on the model performance and also experiment with a subject-specific transfer learning model.\r\n\r\n## Datasets\r\n- Normal MI (Source dataset): [EEG Motor Movement/Imagery Dataset (EEGMMI)](https://physionet.org/content/eegmmidb/1.0.0/)\r\n- Stroke MI (Target dataset): [EEG datasets of stroke patients (Figshare)](https://figshare.com/articles/dataset/EEG_datasets_of_stroke_patients/21679035/3)\r\n"
  },
  {
    "id": "project-4",
    "title": "Film-Buff Chatbot: GenAI-Powered Movie Assistant",
    "image": "/img/project4.png",
    "description": "An interactive chatbot built with Google Gemini that uses a Retrieval-Augmented Generation (RAG) pipeline.",
    "tags": [
      "Python",
      "ChromaDB",
      "RAG"
    ],
    "github": "https://www.kaggle.com/code/sayahashemian/genai",
    "details": "\r\n## Film-Buff Chatbot: Your AI-Powered Movie Assistant\r\n\r\nTired of jumping between different sites for movie trivia, ratings, and good recommendations? This project solves that problem. The **Film-Buff Chatbot** is an interactive, conversational assistant that bundles all those tasks into one place.\r\n\r\n## The Technical Reel: How It's Built\r\n\r\nThe chatbot is built on a multi-stage pipeline that collects, processes, and queries movie data.\r\n\r\n### 1. Building the Knowledge Base\r\n\r\nThe process starts by creating a dynamic and rich dataset:\r\n\r\n- Fetches a list of the **top 250 popular movie titles** using the TMDB API.\r\n- For each title, retrieves detailed metadata including **plot, genre, release year, and IMDb rating** from the OMDb API.\r\n- Cleans the data to handle missing values and duplicates.\r\n- The result is a structured pandas data frame ready for embedding and querying.\r\n\r\n### 2. Enabling Semantic Understanding with a Vector Store\r\n\r\nTo power movie recommendations:\r\n\r\n- Each movie’s **plot summary** is converted into a **numerical embedding** using Google’s `text-embedding-004` model.\r\n- Embeddings are stored in a **ChromaDB vector database**.\r\n- This setup enables **semantic similarity search**, allowing the chatbot to recommend films with similar plots.\r\n\r\n### 3. Poster Analysis with Multimodal Vision\r\n\r\n- Uses **Gemini 1.5 Flash** to \"see\" and analyze movie posters.\r\n- Extracts the **top five dominant hex colors** from poster images.\r\n- Returns a visual **color palette** that reflects the aesthetic tone of the movie.\r\n\r\n### 4. The Conversational Agent & Custom Tools\r\n\r\nThe core of the chatbot is powered by **Gemini 2.0 Flash**, configured as a conversational agent.\r\n\r\nIt leverages custom Python tools to perform complex actions beyond simple text generation:\r\n\r\n#### Key Tools\r\n- `get_movie_info()`: Fetches title, plot, year, and IMDb rating from the database.\r\n- `recommend_movies()`: Finds semantically similar films using ChromaDB embeddings.\r\n- `get_reviews()`: Summarizes recent, real-world reviews using **Google Search grounding**, with citation footnotes.\r\n- `show_palette()`: Extracts and displays the poster’s color palette using multimodal vision.\r\n- Also there are tools for **genre filtering, year search, trailer links**, and **cast lookups**.\r\n\r\n## Tech Stack & Project Resources\r\n\r\n- **LLM & Embeddings:** Google Gemini (2.0 Flash, `text-embedding-004`)\r\n- **Vector Database:** ChromaDB\r\n- **Data Sources:** TMDB API, OMDb API\r\n- **Core Libraries:** Pandas, Matplotlib, Requests\r\n\r\n## Explore the Project\r\n\r\nYou can run the full project notebook on **Kaggle**.\r\n\r\nTo reproduce the results:\r\n\r\n- Runtime: **Kaggle Python 3.10** with **Internet ON**\r\n- Required Secrets:\r\n  - `GOOGLE_API_KEY`\r\n  - `TMDB_API_KEY`\r\n  - `OMDB_API_KEY`\r\n\r\nMake sure to set these secrets using **Kaggle Add-ons → Secrets** before execution."
  }
]